After [building a Keras-like library](http://joelgrus.com/2017/12/04/livecoding-madness-building-a-deep-learning-library/) I got obsessed with figuring out how [autograd](https://en.wikipedia.org/wiki/Automatic_differentiation) works.

Accordingly, here's my toy attempt to build a PyTorch-like library (again, using NumPy).

As usual, there is a [corresponding series of livecoding videos](https://www.youtube.com/watch?v=RxmBukb-Om4&list=PLeDtc0GP5ICldMkRg-DkhpFX1rRBNHTCs) on my YouTube.

* [code for part 1](https://github.com/joelgrus/autograd/tree/part01)
* [code for part 2](https://github.com/joelgrus/autograd/tree/part02)
* [code for part 3](https://github.com/joelgrus/autograd/tree/part03)
* [code for part 4](https://github.com/joelgrus/autograd/tree/part04)
* [code for part 5](https://github.com/joelgrus/autograd/tree/part05)
* [code for part 6](https://github.com/joelgrus/autograd/tree/part06)
